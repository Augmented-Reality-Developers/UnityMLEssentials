Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.0704917,-0.25534976,23.84471718249733,0.21066773521741772,0.21066773521741772,0.096953854,0.0257846,0.000284607,0.19486901,0.004743963,1.0
100000,0.8860914,0.10045976,35.49694189602447,0.29508009168918264,0.29508009168918264,0.058475744,0.025738854,0.0002568077,0.18560258,0.004281568,1.0
150000,0.37599856,1.2920641,77.1094527363184,0.5762354893143448,0.5762354893143448,0.79839534,0.023286687,0.00022590967,0.17530319,0.0037676296,1.0
200000,0.09617062,2.0022035,141.08132530120483,0.872612612640804,0.872612612640804,0.77691597,0.024707291,0.00019487995,0.16495998,0.0032515018,1.0
250000,0.03370789,1.6293106,148.45110410094637,0.9394637224108733,0.9394637224108733,0.26578397,0.02462629,0.00016384674,0.15461557,0.002735316,1.0
300000,0.012703939,1.0657579,154.86970684039088,0.9868403908823911,0.9868403908823911,0.12578312,0.023562696,0.0001327119,0.14423728,0.0022174402,1.0
350000,0.012057135,0.69115585,156.81727574750832,0.9865780730926713,0.9865780730926713,0.0517027,0.023032703,0.00010485293,0.13495097,0.0017540522,1.0
400000,0.011032494,0.5114129,158.22635135135135,0.9931756756771859,0.9931756756771859,0.026039373,0.02417151,7.70294e-05,0.12567644,0.0012912543,1.0
450000,0.013485079,0.46525037,159.61333333333334,0.9966333333340784,0.9966333333340784,0.013439452,0.021649543,4.6126406e-05,0.115375444,0.0007772345,1.0
500000,0.017925791,0.46290106,156.22333333333333,0.9932666666681568,0.9932666666681568,0.009332121,0.023546133,1.5234691e-05,0.105078205,0.0002634021,1.0
